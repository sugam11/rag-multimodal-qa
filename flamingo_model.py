from open_flamingo import create_model_and_transforms
from torch.utils.data import Dataset, DataLoader
from huggingface_hub import hf_hub_download
import torch
from accelerate import Accelerator
from einops import repeat

class FlamingoModel:
    def __init__(self, lang_encoder, tokenizer, n_layers, device=None):
        self.model, self.image_processor, self.tokenizer = create_model_and_transforms(
            clip_vision_encoder_path="ViT-L-14",
            clip_vision_encoder_pretrained="openai",
            lang_encoder_path=lang_encoder,
            tokenizer_path=tokenizer,
            cross_attn_every_n_layers=n_layers,
        )
        self.accelerator = Accelerator()
        self.device = self.accelerator.device
        print(self.device)
        self.args = [lang_encoder, tokenizer, n_layers]
        checkpoint_path = hf_hub_download("openflamingo/OpenFlamingo-4B-vitl-rpj3b-langinstruct", "checkpoint.pt")
        self.model.load_state_dict(torch.load(checkpoint_path), strict=False)
        self.model = self.accelerator.prepare(self.model)
        self.is_main_process = self.accelerator.is_main_process
        self.model.eval()

    """
    Preprocessing images
    Details: For OpenFlamingo, we expect the image to be a torch tensor of shape
     batch_size x num_media x num_frames x channels x height x width.
     In this case batch_size = 1, num_media = 3, num_frames = 1,
     channels = 3, height = 224, width = 224.
    """

    def process_imgs(self, imgs):
        vision_x = [self.image_processor(x).unsqueeze(0) for x in imgs]
        vision_x = torch.cat(vision_x, dim=0)
        vision_x = repeat(vision_x, 'N c h w -> N F c h w', F=1)
        return vision_x.to(self.device)

    """
    Preprocessing text
    Details: In the text we expect an <image> special token to indicate where an image is.
     We also expect an <|endofchunk|> special token to indicate the end of the text
     portion associated with an image.
    """

    def process_text(self, txt):
        self.tokenizer.padding_side = (
            "left"  # For generation padding tokens should be on the left
        )
        if type(txt) == str:
           txt = [txt]
        lang_x = self.tokenizer(
            txt,
            return_tensors="pt",
            padding=True,
            truncation=True
        )
        return lang_x.to(self.device)

    """
    Generate Answer
    Args:
        imgs (list[str]): A list of paths to image inputs to the model
        txt (str):  A string representing the text input to the model
    Outputs:
        answer (str): The answer string generated by the model
    """

    def generate_answer(self, num_beams, imgs, txt):
        vision_x = imgs
        lang_x = txt
        with torch.no_grad():
             generated_text = self.model.generate(
                  vision_x=vision_x,
                  lang_x=lang_x["input_ids"],
                  attention_mask=lang_x["attention_mask"],
                  max_new_tokens=80,
                  early_stopping=True,
                  top_k=0,
                  temperature=0.9,
                  num_return_sequences=1,
                  top_p=0.9,
                  num_beams=num_beams,
               )
        answers = []
        for x in generated_text:
            ans = self.tokenizer.decode(x)
            answers.append(ans)
        return answers
